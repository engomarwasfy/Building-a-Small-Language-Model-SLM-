{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc82b3ed-1c3d-4ba5-a846-304ffd977fee",
   "metadata": {
    "id": "cc82b3ed-1c3d-4ba5-a846-304ffd977fee"
   },
   "source": [
    "# Arabic Small Language Model (SLM) on TinyStories-like Dataset\n",
    "This notebook adapts the TinyStories architecture to train on an Arabic stories dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cc2cc6-3f52-4d44-a2e5-04630c265081",
   "metadata": {
    "id": "38cc2cc6-3f52-4d44-a2e5-04630c265081"
   },
   "outputs": [],
   "source": [
    "!pip install -U datasets tiktoken torch tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f75ccdf-c302-41ed-96c6-181c37d4a529",
   "metadata": {
    "id": "6f75ccdf-c302-41ed-96c6-181c37d4a529"
   },
   "source": [
    "# Loading the Arabic Dataset\n",
    "We will use `arbml/Arabic_Stories_Corpus` or a similar Arabic stories dataset from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c896954-83ae-488b-9c6c-26d13505639d",
   "metadata": {
    "id": "7c896954-83ae-488b-9c6c-26d13505639d"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# Load Arabic stories dataset from HF and ensure train/validation splits\n",
    "raw = load_dataset(\"arbml/Arabic_Stories_Corpus\")\n",
    "if not isinstance(raw, DatasetDict):\n",
    "    raw = DatasetDict({\"train\": raw})\n",
    "\n",
    "if \"validation\" in raw:\n",
    "    dataset = DatasetDict({\"train\": raw[\"train\"], \"validation\": raw[\"validation\"]})\n",
    "elif \"val\" in raw:\n",
    "    dataset = DatasetDict({\"train\": raw[\"train\"], \"validation\": raw[\"val\"]})\n",
    "elif \"test\" in raw:\n",
    "    dataset = DatasetDict({\"train\": raw[\"train\"], \"validation\": raw[\"test\"]})\n",
    "else:\n",
    "    split_ds = raw[\"train\"].train_test_split(test_size=0.1, seed=42, shuffle=True)\n",
    "    dataset = DatasetDict({\"train\": split_ds[\"train\"], \"validation\": split_ds[\"test\"]})\n",
    "\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c337437d-94f7-4c3b-be03-6dd006fd68b0",
   "metadata": {
    "id": "c337437d-94f7-4c3b-be03-6dd006fd68b0"
   },
   "outputs": [],
   "source": [
    "print(df)\n",
    "# Check for column names and rename if necessary to 'text'\n",
    "if 'train' in df:\n",
    "    sample_col = next(iter(df['train'].features.keys()))\n",
    "    if sample_col != 'text':\n",
    "        print(f\"Renaming column '{sample_col}' to 'text'\")\n",
    "        df = df.rename_column(sample_col, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9148fd-bf6a-45fd-ae38-d3dc7031c0e3",
   "metadata": {
    "id": "3f9148fd-bf6a-45fd-ae38-d3dc7031c0e3"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3133a974-d11c-4608-8c77-e1ce8801ed38",
   "metadata": {
    "id": "3133a974-d11c-4608-8c77-e1ce8801ed38"
   },
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5e522b-6a77-48e0-a38d-25e87813f44b",
   "metadata": {
    "id": "4c5e522b-6a77-48e0-a38d-25e87813f44b"
   },
   "source": [
    "Step 1: Tokenization\n",
    "We use `cl100k_base` (used in GPT-4) which has better support for multilingual text (including Arabic) compared to `gpt2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68244ab7-0838-48f7-92fd-239ac8d79db0",
   "metadata": {
    "id": "68244ab7-0838-48f7-92fd-239ac8d79db0"
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import tiktoken\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ad0a97-94b3-4b2f-875f-50809368da47",
   "metadata": {
    "id": "27ad0a97-94b3-4b2f-875f-50809368da47"
   },
   "outputs": [],
   "source": [
    "# Using cl100k_base for better Arabic support\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deef1c94-c812-470c-9ea2-429a79b55817",
   "metadata": {
    "id": "deef1c94-c812-470c-9ea2-429a79b55817"
   },
   "outputs": [],
   "source": [
    "def processing(sample_text):\n",
    "    ids = encoding.encode_ordinary(sample_text['text'])\n",
    "    out = {'ids':ids,'len':len(ids)}\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6a8e1b-5bca-4c47-9929-020c7b533c5a",
   "metadata": {
    "id": "0e6a8e1b-5bca-4c47-9929-020c7b533c5a"
   },
   "outputs": [],
   "source": [
    "cols_to_remove = [c for c in dataset['train'].column_names if c != TEXT_FIELD]\n",
    "if not os.path.exists(\"train.bin\"):\n",
    "    tokenized = dataset.map(\n",
    "        processing,\n",
    "        remove_columns=cols_to_remove,\n",
    "        desc=\"tokenizing the splits\",\n",
    "        num_proc=4,\n",
    "    )\n",
    "    for split, dset in tokenized.items():\n",
    "        arr_len = np.sum(dset['len'], dtype=np.uint64)\n",
    "        filename = f'{split}.bin'\n",
    "        dtype = np.uint32\n",
    "        arr = np.memmap(filename, dtype=dtype, mode='w+', shape=(arr_len,))\n",
    "        total_batches = 1 if len(dset) < 1024 else 1024\n",
    "        idx = 0\n",
    "        for batch_idx in tqdm(range(total_batches), desc=f'writing {filename}'):\n",
    "            batch = dset.shard(num_shards=total_batches, index=batch_idx, contiguous=True).with_format('numpy')\n",
    "            arr_batch = np.concatenate(batch['ids'])\n",
    "            arr[idx : idx + len(arr_batch)] = arr_batch\n",
    "            idx += len(arr_batch)\n",
    "        arr.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1a3f48-5502-443e-a0c2-bfd968a6e3d5",
   "metadata": {
    "id": "7c1a3f48-5502-443e-a0c2-bfd968a6e3d5"
   },
   "source": [
    "Now we will have to create input output pairs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c152fae0-78f5-4e0d-bd93-6224e42eef62",
   "metadata": {
    "id": "c152fae0-78f5-4e0d-bd93-6224e42eef62"
   },
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    if split == 'train':\n",
    "        data = np.memmap('train.bin', dtype=np.uint32, mode='r') # Changed to uint32\n",
    "    else:\n",
    "        # Fallback if validation.bin doesn't exist (some datasets only have train)\n",
    "        if os.path.exists('validation.bin'):\n",
    "             data = np.memmap('validation.bin', dtype=np.uint32, mode='r')\n",
    "        else:\n",
    "             data = np.memmap('train.bin', dtype=np.uint32, mode='r') # Use train for now if no val\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    if device_type == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e119fe00-51e4-4266-825a-bd09f76b729c",
   "metadata": {
    "id": "e119fe00-51e4-4266-825a-bd09f76b729c"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from contextlib import nullcontext\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5899c734-d51b-4ba1-b805-b6c4615e961a",
   "metadata": {
    "id": "5899c734-d51b-4ba1-b805-b6c4615e961a"
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "    def forward(self, x):\n",
    "        return F.layer_norm(x, self.weight.shape, self.weight, self.bias, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25074706-89c0-48f0-a263-cfad629bbc92",
   "metadata": {
    "id": "25074706-89c0-48f0-a263-cfad629bbc92"
   },
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.flash = hasattr(F, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                       .view(1, 1, config.block_size, config.block_size))\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        if self.flash:\n",
    "            y = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.attn_dropout.p if self.training else 0.0, is_causal=True)\n",
    "        else:\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff8bce9-2097-4962-8c69-e8af83a788c9",
   "metadata": {
    "id": "0ff8bce9-2097-4962-8c69-e8af83a788c9"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.c_proj(self.gelu(self.c_fc(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e75443-7d68-43b5-a772-e6b63e5c00f9",
   "metadata": {
    "id": "90e75443-7d68-43b5-a772-e6b63e5c00f9"
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1 = LayerNorm(config.n_embd, config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln2 = LayerNorm(config.n_embd, config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95f97d4-8339-4d8d-96cd-1276a29de712",
   "metadata": {
    "id": "d95f97d4-8339-4d8d-96cd-1276a29de712"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int\n",
    "    vocab_size: int\n",
    "    n_layer: int\n",
    "    n_head: int\n",
    "    n_embd: int\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte=nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe=nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop=nn.Dropout(config.dropout),\n",
    "            h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f=LayerNorm(config.n_embd, config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight  # weight tying\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        if targets is not None:\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "            return logits, loss\n",
    "        else:\n",
    "            logits = self.lm_head(x[:, [-1], :])\n",
    "            return logits, None\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdae0bb-3569-4d24-be5a-05dca1333743",
   "metadata": {
    "id": "9fdae0bb-3569-4d24-be5a-05dca1333743"
   },
   "outputs": [],
   "source": [
    "# Configuration for Arabic Model\n",
    "config = GPTConfig(\n",
    "    vocab_size=100277, # cl100k_base vocab size\n",
    "    block_size=128,\n",
    "    n_layer=6,\n",
    "    n_head=6,\n",
    "    n_embd=384,\n",
    "    dropout=0.1,\n",
    "    bias=True\n",
    ")\n",
    "model = GPT(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53efca0b-52fb-4f3e-a2f1-22cd21e7379b",
   "metadata": {
    "id": "53efca0b-52fb-4f3e-a2f1-22cd21e7379b"
   },
   "outputs": [],
   "source": [
    "eval_iters = 500\n",
    "batch_size = 32\n",
    "block_size = 128\n",
    "gradient_accumulation_steps = 32\n",
    "device =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu'\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "torch.set_default_device(device)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for split in ['train', 'validation']:\n",
    "            if split == 'validation' and not os.path.exists('validation.bin'):\n",
    "                continue\n",
    "            losses = torch.zeros(eval_iters)\n",
    "            for k in range(eval_iters):\n",
    "                X, Y = get_batch(split)\n",
    "                with ctx:\n",
    "                    logits, loss = model(X, Y)\n",
    "                losses[k] = loss.item()\n",
    "            out[split] = losses.mean().item()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "def loss_to_perplexity(loss_value):\n",
    "    return math.exp(loss_value) if math.isfinite(loss_value) else float('inf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f64d17-5f3a-4c86-bc20-78300e3d26d5",
   "metadata": {
    "id": "76f64d17-5f3a-4c86-bc20-78300e3d26d5"
   },
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import LinearLR,SequentialLR, CosineAnnealingLR\n",
    "learning_rate = 1e-4\n",
    "max_iters = 10000\n",
    "warmup_steps = 1000\n",
    "min_lr = 5e-4\n",
    "\n",
    "optimizer =  torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(0.9, 0.95), weight_decay=0.1, eps=1e-9)\n",
    "scheduler_warmup = LinearLR(optimizer, total_iters = warmup_steps)\n",
    "scheduler_decay = CosineAnnealingLR(optimizer,T_max = max_iters - warmup_steps, eta_min = min_lr)\n",
    "scheduler = SequentialLR(optimizer, schedulers=[scheduler_warmup, scheduler_decay], milestones=[warmup_steps])\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fdef4f-ebc7-4c85-9363-2fee3c8dd762",
   "metadata": {
    "id": "53fdef4f-ebc7-4c85-9363-2fee3c8dd762"
   },
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')\n",
    "best_model_params_path = \"best_model_params.pt\"\n",
    "train_loss_list, validation_loss_list = [], []\n",
    "train_ppl_list, validation_ppl_list = [], []\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch in tqdm(range(max_iters)):\n",
    "    if epoch % eval_iters == 0 and epoch != 0:\n",
    "        losses = estimate_loss(model)\n",
    "        train_loss = losses.get('train')\n",
    "        val_loss = losses.get('validation')\n",
    "        train_ppl = loss_to_perplexity(train_loss) if train_loss is not None else float('inf')\n",
    "        val_ppl = loss_to_perplexity(val_loss) if val_loss is not None else float('inf')\n",
    "        msg = f\"Epoch {epoch}: train loss {train_loss:.4f} (ppl {train_ppl:.2f})\"\n",
    "        if val_loss is not None:\n",
    "            msg += f\", val loss {val_loss:.4f} (ppl {val_ppl:.2f})\"\n",
    "            validation_loss_list.append(val_loss)\n",
    "            validation_ppl_list.append(val_ppl)\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(model.state_dict(), best_model_params_path)\n",
    "        print(msg)\n",
    "        train_loss_list.append(train_loss)\n",
    "        train_ppl_list.append(train_ppl)\n",
    "\n",
    "    X, y = get_batch(\"train\")\n",
    "    X, y = X.to(device), y.to(device)\n",
    "    with ctx:\n",
    "        logits, loss = model(X, y)\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "    if ((epoch + 1) % gradient_accumulation_steps == 0) or (epoch + 1 == max_iters):\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Re3jQl7XMox3",
   "metadata": {
    "id": "Re3jQl7XMox3"
   },
   "outputs": [],
   "source": [
    "# Example generation in Arabic\n",
    "sentence = \"كان يا ما كان، كان هناك فتاة صغيرة.\"\n",
    "context = (torch.tensor(encoding.encode_ordinary(sentence)).unsqueeze(dim = 0))\n",
    "context = context.to(device)\n",
    "y = model.generate(context, 200)\n",
    "print(encoding.decode(y.squeeze().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756a1e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation with perplexity\n",
    "if os.path.exists(best_model_params_path):\n",
    "    model.load_state_dict(torch.load(best_model_params_path, map_location=device))\n",
    "losses = estimate_loss(model)\n",
    "for split, val in losses.items():\n",
    "    print(f\"{split} loss: {val:.4f}, ppl: {loss_to_perplexity(val):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CnAHXnWsNRzn",
   "metadata": {
    "id": "CnAHXnWsNRzn"
   },
   "outputs": [],
   "source": [
    "sentence = \"ذهبت فتاة صغيرة إلى الغابة\"\n",
    "context = (torch.tensor(encoding.encode_ordinary(sentence)).unsqueeze(dim = 0))\n",
    "context = context.to(device)\n",
    "y = model.generate(context, 200)\n",
    "print(encoding.decode(y.squeeze().tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
