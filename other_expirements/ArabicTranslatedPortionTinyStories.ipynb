{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c33fb7d5",
   "metadata": {},
   "source": [
    "# Arabic TinyStories (Translated) SLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5523858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/wasfy/anaconda3/envs/slm/lib/python3.8/site-packages (3.1.0)\n",
      "Requirement already satisfied: transformers in /home/wasfy/anaconda3/envs/slm/lib/python3.8/site-packages (4.46.3)\n",
      "Requirement already satisfied: sentencepiece in /home/wasfy/anaconda3/envs/slm/lib/python3.8/site-packages (0.2.0)\n",
      "Requirement already satisfied: tqdm in /home/wasfy/anaconda3/envs/slm/lib/python3.8/site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in /home/wasfy/anaconda3/envs/slm/lib/python3.8/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/wasfy/anaconda3/envs/slm/lib/python3.8/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: aiohttp in /home/wasfy/anaconda3/envs/slm/lib/python3.8/site-packages (from datasets) (3.10.11)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/wasfy/anaconda3/envs/slm/lib/python3.8/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]<=2024.9.0,>=2023.1.0 in /home/wasfy/anaconda3/envs/slm/lib/python3.8/site-packages (from datasets) (2024.9.0)\n",
      "Requirement already satisfied: pandas in /home/wasfy/anaconda3/envs/slm/lib/python3.8/site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /home/wasfy/anaconda3/envs/slm/lib/python3.8/site-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in /home/wasfy/anaconda3/envs/slm/lib/python3.8/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/wasfy/anaconda3/envs/slm/lib/python3.8/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/wasfy/anaconda3/envs/slm/lib/python3.8/site-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: xxhash in /home/wasfy/anaconda3/envs/slm/lib/python3.8/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/wasfy/anaconda3/envs/slm/lib/python3.8/site-packages (from datasets) (1.23.5)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/wasfy/anaconda3/envs/slm/lib/python3.8/site-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/wasfy/anaconda3/envs/slm/lib/python3.8/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/wasfy/anaconda3/envs/slm/lib/python3.8/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/wasfy/anaconda3/envs/slm/lib/python3.8/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/wasfy/anaconda3/envs/slm/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/wasfy/anaconda3/envs/slm/lib/python3.8/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/wasfy/anaconda3/envs/slm/lib/python3.8/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/wasfy/anaconda3/envs/slm/lib/python3.8/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/wasfy/anaconda3/envs/slm/lib/python3.8/site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /home/wasfy/anaconda3/envs/slm/lib/python3.8/site-packages (from aiohttp->datasets) (1.15.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/wasfy/anaconda3/envs/slm/lib/python3.8/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/wasfy/anaconda3/envs/slm/lib/python3.8/site-packages (from huggingface-hub>=0.23.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/wasfy/anaconda3/envs/slm/lib/python3.8/site-packages (from huggingface-hub>=0.23.0->datasets) (4.13.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/wasfy/anaconda3/envs/slm/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/wasfy/anaconda3/envs/slm/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/wasfy/anaconda3/envs/slm/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/wasfy/anaconda3/envs/slm/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (1.26.14)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/wasfy/anaconda3/envs/slm/lib/python3.8/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/wasfy/anaconda3/envs/slm/lib/python3.8/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/wasfy/anaconda3/envs/slm/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/wasfy/anaconda3/envs/slm/lib/python3.8/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U datasets transformers sentencepiece tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bab4a95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce RTX 4070\n",
      "GPU 1: NVIDIA GeForce RTX 3080 Ti\n",
      "GPU 2: NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "# List available CUDA devices\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"CUDA not available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0fe1d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using devices for translation: [0, 1]; train_limit=2119, val_limit=21\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36f33d773cc345b2a71f0a26462ce17b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "translating:   0%|          | 0/1060 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your input_length: 512 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 512 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 492 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 512 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 512 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 512 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 495 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 512 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 464 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 512 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 512 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 512 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 491 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 512 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 512 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 464 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 512 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 512 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 512 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 477 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 512 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 512 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 512 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 512 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 512 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 512 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 498 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 479 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 505 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 495 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 512 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 512 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 497 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 510 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 482 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 512 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 512 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 464 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 512 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 512 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 500 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 512 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 512 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 512 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 512 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 495 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 512 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 512 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 512 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 512 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 512 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 512 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 507 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 512 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 512 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 478 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 512 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 506 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 465 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 512 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 512 is bigger than 0.9 * max_length: 512. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "925e91c625084aac9d9c2d52ee838f89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "translating:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fdd0280f23a495bbc1d5ebcb0e31be3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94565957ef0040ed8001027fe62fef6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 2119\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 21\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import os, math, torch\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Translate 1% as proof of concept; cache to parquet (Marian en->ar)\n",
    "TRANSLATE_FRAC = 0.001\n",
    "CACHE_TRAIN = \"tini_ar_train.parquet\"\n",
    "CACHE_VAL = \"tini_ar_val.parquet\"\n",
    "BATCH_SIZE_TRANSLATE = 2\n",
    "MAX_LENGTH = 512\n",
    "DEVICES = [0, 1]  # GPUs to use; CPU fallback\n",
    "MODEL_NAME = \"Helsinki-NLP/opus-mt-en-ar\"\n",
    "\n",
    "\n",
    "def load_and_translate():\n",
    "    if os.path.exists(CACHE_TRAIN) and os.path.exists(CACHE_VAL):\n",
    "        train_ds = Dataset.from_parquet(CACHE_TRAIN)\n",
    "        val_ds = Dataset.from_parquet(CACHE_VAL)\n",
    "        print(\"Loaded cached translated datasets.\")\n",
    "        return DatasetDict({\"train\": train_ds, \"validation\": val_ds})\n",
    "\n",
    "    tiny = load_dataset(\"roneneldan/TinyStories\")\n",
    "    train_limit = max(1, int(len(tiny['train']) * TRANSLATE_FRAC))\n",
    "    val_limit = max(1, int(len(tiny['validation']) * TRANSLATE_FRAC))\n",
    "\n",
    "    available = torch.cuda.device_count()\n",
    "    devices = [d for d in DEVICES if d < available] if torch.cuda.is_available() else []\n",
    "    if not devices:\n",
    "        devices = [-1]\n",
    "    pipes = [pipeline(\"translation\", model=MODEL_NAME, device=d) for d in devices]\n",
    "    print(f\"Using devices for translation: {devices}; train_limit={train_limit}, val_limit={val_limit}\")\n",
    "\n",
    "    def translate_split(split_ds, limit):\n",
    "        texts = split_ds[\"text\"][:limit]\n",
    "        outputs_all = []\n",
    "        pcount = len(pipes)\n",
    "        for chunk_start in tqdm(range(0, len(texts), BATCH_SIZE_TRANSLATE), desc=\"translating\"):\n",
    "            chunk = texts[chunk_start:chunk_start+BATCH_SIZE_TRANSLATE]\n",
    "            pipe = pipes[(chunk_start//BATCH_SIZE_TRANSLATE) % pcount]\n",
    "            outs = pipe(chunk, max_length=MAX_LENGTH, truncation=True)\n",
    "            outputs_all.extend([o[\"translation_text\"] for o in outs])\n",
    "        return Dataset.from_dict({\"text\": outputs_all})\n",
    "\n",
    "    train_ds = translate_split(tiny['train'], train_limit)\n",
    "    val_ds = translate_split(tiny['validation'], val_limit)\n",
    "    train_ds.to_parquet(CACHE_TRAIN)\n",
    "    val_ds.to_parquet(CACHE_VAL)\n",
    "    return DatasetDict({\"train\": train_ds, \"validation\": val_ds})\n",
    "\n",
    "dataset = load_and_translate()\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a14903",
   "metadata": {},
   "source": [
    "Tokenize with Arabic GPT-2 tokenizer and bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "705d6574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffcae2b9bd564b0d9a8a2077e455a645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizing (num_proc=4):   0%|          | 0/2119 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e78f09694b324b1da52a1327c8bf74c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizing (num_proc=4):   0%|          | 0/21 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0b153af87b544b3b3f6e80d93bcdb4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "writing tini_ar_train.bin:   0%|          | 0/512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be60ddd3f10f446aa673f271cc31f002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "writing tini_ar_validation.bin:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"aubmindlab/aragpt2-base\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "BIN_PREFIX = \"tini_ar\"\n",
    "TRAIN_BIN = f\"{BIN_PREFIX}_train.bin\"\n",
    "VAL_BIN = f\"{BIN_PREFIX}_validation.bin\"\n",
    "\n",
    "\n",
    "def processing(sample_text):\n",
    "    ids = tokenizer.encode(sample_text['text'], add_special_tokens=False)\n",
    "    return {'ids': ids, 'len': len(ids)}\n",
    "\n",
    "cols_to_remove = [c for c in dataset['train'].column_names if c != 'text']\n",
    "if not (os.path.exists(TRAIN_BIN) and os.path.exists(VAL_BIN)):\n",
    "    tokenized = dataset.map(processing, remove_columns=cols_to_remove, desc=\"tokenizing\", num_proc=4)\n",
    "    for split, dset in tokenized.items():\n",
    "        arr_len = np.sum(dset['len'], dtype=np.uint64)\n",
    "        filename = TRAIN_BIN if split == 'train' else VAL_BIN\n",
    "        arr = np.memmap(filename, dtype=np.uint32, mode='w+', shape=(arr_len,))\n",
    "        total_batches = 1 if len(dset) < 512 else 512\n",
    "        idx = 0\n",
    "        for b in tqdm(range(total_batches), desc=f\"writing {filename}\"):\n",
    "            shard = dset.shard(num_shards=total_batches, index=b, contiguous=True).with_format('numpy')\n",
    "            arr_batch = np.concatenate(shard['ids'])\n",
    "            arr[idx: idx+len(arr_batch)] = arr_batch\n",
    "            idx += len(arr_batch)\n",
    "        arr.flush()\n",
    "else:\n",
    "    print(\"Reusing existing bins\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5532caa",
   "metadata": {},
   "source": [
    "Batches and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "887ed545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "\n",
    "def get_batch(split, block_size=128, batch_size=16):\n",
    "    data = np.memmap(TRAIN_BIN if split=='train' else VAL_BIN, dtype=np.uint32, mode='r')\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__(); self.weight=nn.Parameter(torch.ones(ndim)); self.bias=nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "    def forward(self,x): return F.layer_norm(x,self.weight.shape,self.weight,self.bias,1e-5)\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(); assert config.n_embd % config.n_head==0\n",
    "        self.c_attn=nn.Linear(config.n_embd,3*config.n_embd,bias=config.bias)\n",
    "        self.c_proj=nn.Linear(config.n_embd,config.n_embd,bias=config.bias)\n",
    "        self.attn_dropout=nn.Dropout(config.dropout); self.resid_dropout=nn.Dropout(config.dropout)\n",
    "        self.n_head=config.n_head; self.n_embd=config.n_embd\n",
    "    def forward(self,x):\n",
    "        B,T,C=x.size(); q,k,v=self.c_attn(x).split(self.n_embd,dim=2)\n",
    "        k=k.view(B,T,self.n_head,C//self.n_head).transpose(1,2)\n",
    "        q=q.view(B,T,self.n_head,C//self.n_head).transpose(1,2)\n",
    "        v=v.view(B,T,self.n_head,C//self.n_head).transpose(1,2)\n",
    "        att=(q@k.transpose(-2,-1))/math.sqrt(k.size(-1)); att=F.softmax(att,dim=-1); att=self.attn_dropout(att)\n",
    "        y=att@v; y=y.transpose(1,2).contiguous().view(B,T,C); y=self.resid_dropout(self.c_proj(y)); return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(); self.c_fc=nn.Linear(config.n_embd,4*config.n_embd,bias=config.bias)\n",
    "        self.gelu=nn.GELU(); self.c_proj=nn.Linear(4*config.n_embd,config.n_embd,bias=config.bias); self.dropout=nn.Dropout(config.dropout)\n",
    "    def forward(self,x): return self.dropout(self.c_proj(self.gelu(self.c_fc(x))))\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(); self.ln1=LayerNorm(config.n_embd, config.bias); self.attn=CausalSelfAttention(config)\n",
    "        self.ln2=LayerNorm(config.n_embd, config.bias); self.mlp=MLP(config)\n",
    "    def forward(self,x): x=x+self.attn(self.ln1(x)); x=x+self.mlp(self.ln2(x)); return x\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size:int; vocab_size:int; n_layer:int; n_head:int; n_embd:int; dropout:float=0.1; bias:bool=True\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(); self.config=config\n",
    "        self.transformer=nn.ModuleDict(dict(\n",
    "            wte=nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe=nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop=nn.Dropout(config.dropout),\n",
    "            h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f=LayerNorm(config.n_embd, config.bias),\n",
    "        ))\n",
    "        self.lm_head=nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight=self.lm_head.weight\n",
    "        self.apply(self._init_weights)\n",
    "        for pn,p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'): nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2*config.n_layer))\n",
    "    def _init_weights(self,m):\n",
    "        if isinstance(m, nn.Linear): nn.init.normal_(m.weight, mean=0.0, std=0.02); nn.init.zeros_(m.bias) if m.bias is not None else None\n",
    "        elif isinstance(m, nn.Embedding): nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "    def forward(self, idx, targets=None):\n",
    "        b,t=idx.size(); pos=torch.arange(0,t,device=idx.device)\n",
    "        x=self.transformer.drop(self.transformer.wte(idx)+self.transformer.wpe(pos))\n",
    "        for block in self.transformer.h: x=block(x)\n",
    "        x=self.transformer.ln_f(x)\n",
    "        if targets is not None:\n",
    "            logits=self.lm_head(x); loss=F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "            return logits, loss\n",
    "        logits=self.lm_head(x[:,[-1],:]); return logits, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2888f0",
   "metadata": {},
   "source": [
    "Training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d5df02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_773028/3511579663.py:21: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(device=='cuda'))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4868e2009857499eb5002a4e18bdb607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wasfy/anaconda3/envs/slm/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 200: train 7.2096 (ppl 1352.32) val 7.0644 (ppl 1169.63)\n",
      "step 400: train 5.8562 (ppl 349.39) val 5.7601 (ppl 317.39)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wasfy/anaconda3/envs/slm/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:232: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 600: train 5.2702 (ppl 194.45) val 5.2235 (ppl 185.59)\n",
      "step 800: train 4.8854 (ppl 132.35) val 4.8681 (ppl 130.08)\n",
      "step 1000: train 4.5883 (ppl 98.33) val 4.6300 (ppl 102.52)\n",
      "step 1200: train 4.2988 (ppl 73.61) val 4.4125 (ppl 82.47)\n",
      "step 1400: train 3.4663 (ppl 32.02) val 3.5902 (ppl 36.24)\n",
      "step 1600: train 0.9656 (ppl 2.63) val 0.9072 (ppl 2.48)\n",
      "step 1800: train 0.4425 (ppl 1.56) val 0.4313 (ppl 1.54)\n",
      "step 2000: train 0.2382 (ppl 1.27) val 0.2765 (ppl 1.32)\n",
      "step 2200: train 0.1565 (ppl 1.17) val 0.1994 (ppl 1.22)\n",
      "step 2400: train 0.1115 (ppl 1.12) val 0.1672 (ppl 1.18)\n",
      "step 2600: train 0.0889 (ppl 1.09) val 0.1461 (ppl 1.16)\n",
      "step 2800: train 0.0720 (ppl 1.07) val 0.1329 (ppl 1.14)\n",
      "step 3000: train 0.0620 (ppl 1.06) val 0.1253 (ppl 1.13)\n",
      "step 3200: train 0.0555 (ppl 1.06) val 0.1217 (ppl 1.13)\n",
      "step 3400: train 0.0510 (ppl 1.05) val 0.1173 (ppl 1.12)\n",
      "step 3600: train 0.0473 (ppl 1.05) val 0.1132 (ppl 1.12)\n",
      "step 3800: train 0.0451 (ppl 1.05) val 0.1101 (ppl 1.12)\n",
      "step 4000: train 0.0436 (ppl 1.04) val 0.1092 (ppl 1.12)\n",
      "step 4200: train 0.0420 (ppl 1.04) val 0.1058 (ppl 1.11)\n",
      "step 4400: train 0.0403 (ppl 1.04) val 0.1056 (ppl 1.11)\n",
      "step 4600: train 0.0400 (ppl 1.04) val 0.1058 (ppl 1.11)\n",
      "step 4800: train 0.0389 (ppl 1.04) val 0.1005 (ppl 1.11)\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "config = GPTConfig(vocab_size=tokenizer.vocab_size, block_size=128, n_layer=6, n_head=6, n_embd=384)\n",
    "model = GPT(config).to(device)\n",
    "\n",
    "base_lr = 3e-4\n",
    "batch_size = 16\n",
    "block_size = config.block_size\n",
    "gradient_accumulation_steps = 4\n",
    "lr_scale = batch_size/8.0\n",
    "learning_rate = base_lr * max(lr_scale, 1e-2)\n",
    "min_lr = learning_rate*0.1\n",
    "max_iters = 5000\n",
    "warmup_steps = 500\n",
    "eval_iters = 200\n",
    "\n",
    "from torch.optim.lr_scheduler import LinearLR, SequentialLR, CosineAnnealingLR\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(0.9,0.95), weight_decay=0.1, eps=1e-9)\n",
    "scheduler_warmup = LinearLR(optimizer, total_iters=warmup_steps)\n",
    "scheduler_decay = CosineAnnealingLR(optimizer, T_max=max_iters-warmup_steps, eta_min=min_lr)\n",
    "scheduler = SequentialLR(optimizer, schedulers=[scheduler_warmup, scheduler_decay], milestones=[warmup_steps])\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(device=='cuda'))\n",
    "\n",
    "\n",
    "def estimate_loss():\n",
    "    out={}; model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for split in ['train','validation']:\n",
    "            losses=torch.zeros(eval_iters)\n",
    "            for k in range(eval_iters):\n",
    "                X,Y=get_batch(split, block_size=block_size, batch_size=batch_size)\n",
    "                with torch.amp.autocast(device_type='cuda' if device=='cuda' else 'cpu', dtype=torch.float16 if device=='cuda' else torch.float32):\n",
    "                    logits, loss = model(X, Y)\n",
    "                losses[k]=loss.item()\n",
    "            out[split]=losses.mean().item()\n",
    "    model.train(); return out\n",
    "\n",
    "def loss_to_ppl(val): return math.exp(val) if math.isfinite(val) else float('inf')\n",
    "\n",
    "best_val=float('inf')\n",
    "best_path='best_tini_ar.pt'\n",
    "train_losses=[]; val_losses=[]; train_ppl=[]; val_ppl=[]\n",
    "\n",
    "for step in tqdm(range(max_iters)):\n",
    "    if step % eval_iters == 0 and step>0:\n",
    "        losses=estimate_loss()\n",
    "        t,v = losses['train'], losses['validation']\n",
    "        train_losses.append(t); val_losses.append(v)\n",
    "        train_ppl.append(loss_to_ppl(t)); val_ppl.append(loss_to_ppl(v))\n",
    "        print(f\"step {step}: train {t:.4f} (ppl {train_ppl[-1]:.2f}) val {v:.4f} (ppl {val_ppl[-1]:.2f})\")\n",
    "        if v < best_val:\n",
    "            best_val=v; torch.save(model.state_dict(), best_path)\n",
    "    X,Y=get_batch('train', block_size=block_size, batch_size=batch_size)\n",
    "    with torch.amp.autocast(device_type='cuda' if device=='cuda' else 'cpu', dtype=torch.float16 if device=='cuda' else torch.float32):\n",
    "        logits, loss = model(X,Y)\n",
    "        loss = loss/gradient_accumulation_steps\n",
    "    scaler.scale(loss).backward()\n",
    "    if (step+1)%gradient_accumulation_steps==0:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        scaler.step(optimizer); scaler.update(); optimizer.zero_grad(set_to_none=True)\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf9b0b4",
   "metadata": {},
   "source": [
    "Generation from best checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "187d1133",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_773028/1563840041.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_tini_ar.pt', map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "في قديم الزمان كان هناك طفل صغير يحب القصص. يحب القصص قديم الزمان القصص الزمان القصص القصص القصص الزمان القصص قديم الزمان القصصاذ القصص قديم قديم قديم قديم مليء القصص قديم قديم قديم قديم قديم قديم قديم القصص القصص الزمان القصص قديم قديم القصص القصص القصص القصص قديم قديم قديم الزمان القصص قديم قديم قديم الزمان الزمان القصص\n",
      "كانت الطفلة تمشي في الغابة وتسمع الطيور تغني. الطيور تغني. في الغابة في الطيور تغني في الغابة في الغابة تمشي في الغابة وتس وتس تمشي في الغابة وتس الطيور في الغابة الطيور الغابة. في الغابة الغابة في الغابة في الغابة الغابة الغابة الغابة في الغابة الطفلة الغابة الغابة الغابة في الغابة الغابة الغابة الغابة في الغابة\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best_tini_ar.pt', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "def generate(prompt, max_new_tokens=50, temperature=0.8, top_k=50):\n",
    "    idx = tokenizer.encode(prompt, add_special_tokens=False, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx if idx.size(1) <= config.block_size else idx[:, -config.block_size:]\n",
    "            logits,_ = model(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_k is not None:\n",
    "                v,_ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('inf')\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_id = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, next_id), dim=1)\n",
    "    return tokenizer.decode(idx[0].tolist())\n",
    "\n",
    "print(generate(\"في قديم الزمان كان هناك طفل صغير يحب القصص.\"))\n",
    "print(generate(\"كانت الطفلة تمشي في الغابة وتسمع الطيور تغني.\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
